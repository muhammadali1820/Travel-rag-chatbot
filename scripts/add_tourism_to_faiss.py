# scripts/add_tourism_to_faiss.py

# âœ… Step 1 â€” Ensure we can import app/ modules
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# âœ… Step 2 â€” Imports
import faiss
import pickle
from app.utils import save_pickle, load_pickle

# âœ… Step 3 â€” Load the embeddings we created
with open("data/tourism_embeddings.pkl", "rb") as f:
    embeddings = pickle.load(f)

print(f"ğŸ“¦ Loaded {len(embeddings)} embeddings from file.")

# âœ… Step 4 â€” Create or load FAISS index
store_path = "store/faiss.index"
if not os.path.exists("store"):
    os.makedirs("store")

try:
    index = faiss.read_index(store_path)
    print("ğŸ“ Existing FAISS index found â€” appending new data...")
except:
    print("ğŸ†• No existing FAISS index found â€” creating new one...")
    # Create a new FAISS index based on the embedding dimension
    index = faiss.IndexFlatL2(embeddings[0].shape[0])

# âœ… Step 5 â€” Add the embeddings to the index
index.add(embeddings)
print(f"âœ… Added {len(embeddings)} vectors to the FAISS index.")

# âœ… Step 6 â€” Save updated FAISS index
faiss.write_index(index, store_path)
print("ğŸ’¾ FAISS index saved successfully!")

# âœ… Step 7 â€” Save a docstore (to map each vector â†’ original text)
chunks_path = "data/tourism_text_chunks.txt"
with open(chunks_path, "r", encoding="utf-8") as f:
    chunks = f.read().split("---CHUNK-END---")

chunks = [chunk.strip() for chunk in chunks if chunk.strip()]

docstore_path = "store/docstore.pkl"
try:
    docstore = load_pickle(docstore_path)
except:
    docstore = {}

start_id = len(docstore)
for i, chunk in enumerate(chunks):
    docstore[start_id + i] = chunk

save_pickle(docstore, docstore_path)
print(f"ğŸ§¾ Docstore updated with {len(chunks)} chunks.")
print("ğŸ‰ All done! Your FAISS index and docstore are ready.")
